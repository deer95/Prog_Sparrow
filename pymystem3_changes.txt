# Оригинальный pymystem3 работает так:
#   -на вход принимается текст в переменной типа str.
#   -pymystem3 делит этот текст на строки (splitlines ()), и скармливает его Mystem-у построчно, осуществляя столько вызовов Mystem-а, сколько оказалось в тексте строк.
#   -Mystem выплёвывает данные в формате json, который превращается специальной читалкой в python-структуру.
#
#
# Проблемы оригинального pymystem3 v0.1.9 :
#   -1: оригинальные функции analyze/lemmatize принимают на вход уже прочитанный текст для разбора (в переменной типа str). И нет возможности передать Mystem путь к файлу, чтобы Mystem сам его открыл, прочёл и разобрал (при том, что сам Mystem умеет работать в двух режимах: читать из стандартного ввода, или же читать из указаного в качестве аргумента файла).
#   -2: pymystem3 работает невыносимо медленно (под Windows как минимум), если передавать для анализа текст с большим кол-вом строчек (с большим кол-вом \n). Дело в том, что pymystem3 осуществляет отдельный вызовов Mystem-а для каждой строки, а вызов Mystem-a - дорогая операция: работает он быстро, но загружается долго. По идее, надо было бы убрать splitlines и скармливать весь текст за один вызов, но возникают ещё две проблемы (из-за которых это не было сделано):
#   -3: правильная конвертация json-читалкой вывода Mystem, если вход состоит из нескольких строк (раньше ведь строка всегда была одна, а хочется уметь передавать файлы, а в них есть переносы). В этом случае разбор каждой строки будет записан в отдельной строке. json-читалка такое кушать не умеет, её надо было бы тогда вызывать для каждой строки отдельно, иначе на символах '\r\n' она падает (потому что синтаксически вывод представляет из себя множество json-структур, разделённых \r\n, по одной на каждой строке, а не одну, как было раньше).
#   -4: сам Mystem иногда виснет, когда с stdin-а ему приходит текст из нескольких достаточно больших строк (есть пример таких файлов). Если при этом убрать все переносы строк - Mystem отрабатывает нормально. При чтении из файла таких проблем нет.
#
#
# Решения проблем и улучшения:
#   - добавлен костыль, который склеивает строки вывода json-формата, что позволило обрабатывать файлы из нескольких строк. Кроме того, при entire_input = False возникают проблемы из-за того, что в выводе Mystem-а в json формате появляются конструкции типа []\r\n[]\r\n[... оставшиеся от пустых строк (содержащих только разделители). По идее, если вызывать Mystem с ключом -n, то потом всё это собрать в json-структуру проще - но работает дольше, чем текущий вариант.
#   - аргумент speedup для lemmatize/analyze: добавлен способ обработать текст (переданный через аргумент text) за один вызов Mystem-а (по умолчанию speedup == False). Специальным образом переносы строк удаляются и восстанавливаются после.
#   - аргумент file_path для lemmatize/analyze: можно указать путь к файлу, который будет передан Mystem-у при запуске как аргумент командной строки. Аргументы text и speedup игнорируются, если указан file_path. Файл должен быть в кодировке utf-8 (выбор кодировки мне было лень поддерживать). Путь к файлу не должен содержать кириллических символов (Mystem с таким не умеет работать). Осуществляется только один запуск Mystem-а, работает быстро.
#   - поддержаны дополнительные опции, которые можно передать Mystem-у (не поддержанные изначально в pymystem3). Полный список поддерживаемых опций - ниже.
#   - функция get_printable_repr: выдаёт вам строку, в которой красиво напечатан результат разбора для одного токена.
#
######## Usage:
#pymystem3.Mystem (mystem_bin=None,
#                  grammar_info=True,
#                  disambiguation=True,
#                  entire_input=True,
#                  glue_gr_info=True,
#                  eng_gr=False,
#                  weight=False,
#                  no_bastards=False,
#                  generate_all=False)
#
###### original supported options:
## <in Mystem>     <in pymystem3 (=default)> - <description>
#                  mystem_bin     (=None) - путь к бинарнику, если вы не хотите использвать по умолчанию: %UserProfile%/.local/bin/mystem
# -d               disambiguation (=True) - применить контекстное снятие частиречной омонимии.
# -с               entire_input   (=True) - включать в вывод всё: разделители, пунктуаторы, числа... Но: токены, содержащие латинские символы (английские слова, римские цифры...) включаются в вывод всегда, хотя и не имеет анализа. А если токен содержит хотя бы одну цифру - то он будет исключён из вывода, если эта опция включена.
# -i               grammar_info   (=True) - возвращать грамматический тег (помимо леммы, которая возвращается всегда для токенов, подлежащих разбору).

###### additional supported options:
# -g               glue_gr_info   (=True) - склеивать гр.информацию при одной лемме в случае грамматической неоднозначности (с помощью "(var1|var2)"), (только при включенной опции -i).
# --eng-gr         eng_gr        (=False) - Print grammems in English
# --weight         weight        (=False) - Print context-independent lemma weight
# -w               no_bastards   (=False) - разбирать только словарные слова (для несловарных analysis будет пустым)
# --generate_all   generate_all  (=False) - генерировать все возможные гипотезы для несловарных слов
#
# Все остальные опции не поддержаны, или нерелевантны для вызова из pymystem3.
#
#
###### not supported or not applicable options:
# -h, -?                 NOT APPLICABLE   - Print synopsis and exit
# -v                     NOT APPLICABLE   - Print current version and exit   
# -n                     NOT APPLICABLE   - Print every word on a new line (не играет роли для pymystem3)
# -l                     NOT APPLICABLE   - не печатать исходные словоформы,  только леммы и граммемы (при выводе в json опция не поддержана)
# -f                     NOT SUPPORTED    - Print lemma frequency. (Deprecated. Use --weight)
# -s                     NOT APPLICABLE   - Print end of sentence mark (works only with -c) (принцип работы неясен) 
# -e <encoding>          NOT SUPPORTED    - Specify input/output encoding (UTF-8 by default)  
# --filter-gram <list>   NOT SUPPORTED    - List of accepted grammemes (comma-separated) (Строить разборы только с указанными граммемами)
# --fixlist <file>       NOT SUPPORTED    - Использовать файл с пользовательским словарём
# --format <format>      NOT APPLICABLE   - Output format: text(default), xml, json (для pymystem3 требуется вывод в json)
# 
###### function calls:
# analyze (text, file_path='', speedup=False)
# lemmatize (text, file_path='', speedup=False)
#
# Аргументы:
#   text      - текст, который необходимо распарсить;
#   speedup   - обработать текст за один вызов (костыль для ускорения работы);
#   file_path - если определить file_path, то будет произведено чтение из файла в кодировке utf-8, переменная text будет проигнорирована.
#
#
###### как устроена структура результата разбора:
#
# Структура, которую возвращает функция analyze, является, грубо говоря, списком словарей списков словарей. А точнее:
# - функция analyze возвращает список токенов;
# - каждый токен является структурой типа dict с двумя ключами:
# -- 'text' - соответствующие токену символы в тексте в том виде, в котором они встретились в тексте;
# -- 'analysis' - список возможных вариантов разбора этого токена. При этом:
# ---- токены, являющиеся разделителями, пунктуацией, межсловными промежутками и те, что содержат цифры - не подлежат разбору и не содержат ключа 'analysis', и могут быть исключены из результатов с помощью опции entire_input;
# ---- токены, содержащие нерусские буквы (английские слова, римские цифры...) также не подлежат разбору, хотя и содержат ключ 'analysis', но список вариантов для таких токенов пуст;
# ---- для остальных токенов, если включены опции disambiguation и glue_gr_info, список должен содержать ровно один вариант разбора (что, впрочем, иногда неверно из-за странностей в работе Mystem, особенно для несловарных слов, или, например, для слова "тыкать");
# ---- каждый вариант разбора представляет себя словарь со следующими ключами:
# ----- 'lex' - лемма;
# ----- 'gr' - грамматический тег для этой леммы (если включена опция grammar_info);
# ----- 'wt' - вес леммы, не зависящий от контекста (если включена опция weight);
# ----- 'qual' - метка несловарного слова (только для несловарных слов);